<!--
SPDX-FileCopyrightText: 2025 robot-visual-perception

SPDX-License-Identifier: CC-BY-4.0
-->

# OptiBot

Real-time visual perception system for robots combining object detection (YOLO) and monocular depth estimation (MiDaS) over low-latency WebRTC streams.

## Quick Start (Windows)

For Windows users, we provide an automated setup script:
```powershell
PowerShell -ExecutionPolicy Bypass -File .\start-optibot.ps1
```

This script will:
- Install all required dependencies (Python 3.11, Node.js, Git, Make, uv)
- Build the project
- Start all services automatically
- Open your browser to http://localhost:3000

**Requirements:** Administrator privileges

For manual setup or other platforms, see the sections below.

## What’s included
- FastAPI + aiortc backend streaming webcam frames
- YOLOv8n inference (Ultralytics) on the server
- Rough monocular distance estimate per detection
- WebRTC DataChannel sending metadata to the client
- React + Vite frontend showing the remote stream and detection stats

## Run with Docker Compose (Linux only)

**Note:** Camera access in Docker only works on Linux.

```bash
make docker-compose-up
```

## Run locally (All platforms)

Prereqs: Python 3.11

1) Install dependencies
```
make dev
```
2) Start the webcam service
```
make run-streamer-webcam
```
Or alternatively, if you provide a .mp4 file in the `/backend` folder per default, you can start the file service instead
```
make run-streamer-file
```
3) Start the analyzer service (separate terminal)
```
make run-analyzer-local
```
4) Start the frontend service (separate terminal)
```
make run-frontend-local
```
Open the shown URL in your console.

## Model Management
```bash
# Download default models (YOLO pt, MiDaS cache)
make download-models

# Download and export to ONNX
make download-models-onnx

# Download individual models
make download-yolo
make download-midas
make download-depth-anything
make download-depth-pro

# Export models to ONNX
make export-yolo-onnx
make export-midas-onnx
```

To start the analyzer service with ONNX backend:
```bash
DETECTOR_BACKEND=onnx DEPTH_BACKEND=onnx make run-analyzer-local
```


To start the analyzer service with Depth Anything V2 backend:
```bash
DEPTH_BACKEND=depth_anything_v2 make run-analyzer-local
```

To start the analyzer service with Apple's ML Depth Pro backend:
```bash
DEPTH_BACKEND=depth_pro make run-analyzer-local
```
*Note: Depth Pro model weights are approx. 1.8 GB and will be downloaded automatically (or via `make download-depth-pro`).*

Example production usage with custom model type:
```bash
# Set model type via environment variable
MIDAS_MODEL_TYPE=DPT_Hybrid \
cd src/backend && uv run python -m analyzer.cli \
  --yolo-model-path ./models/yolo11n.pt \
  --midas-model-path ./models/midas_cache
```

Available CLI flags:
- `--yolo-model-path`: Path to YOLO model file (e.g., `yolo11n.pt`; `yolov8n.pt` still works)
- `--midas-model-path`: Path to MiDaS model cache directory
- `--host`: Host to bind to (default: `0.0.0.0`)
- `--port`: Port to bind to (default: `8001`)
- `--reload`: Enable auto-reload for development

### Environment Variables

Optional environment variables:
- `CAMERA_INDEX` (default 0) – select webcam device
- `REGION_SIZE` (default 5) – size of the central bounding box region where we take the mean of the depth map from (should be odd for symmetry)
- `SCALE_FACTOR` (default 432.0) – scaling of the relative depth map generated by MiDaS (must be determined empirically)
- `CAMERA_FX/FY/CX/CY` – intrinsic matrix entries in pixels (set these when you have calibrated your camera; overrides FOV-derived values)
- `CAMERA_FOV_X_DEG/CAMERA_FOV_Y_DEG` – fallback field of view (used only when FX/FY are not provided)
- `DEPTH_BACKEND` – `torch` (default), `onnx`, or `depth_anything_v2`
- `MIDAS_MODEL_TYPE` – MiDaS variant to load (`MiDaS_small`, `DPT_Hybrid`, `DPT_Large`)
- `MIDAS_MODEL_REPO` – torch.hub repo for MiDaS (default `intel-isl/MiDaS`)
- `DEPTH_ANYTHING_MODEL` – Hugging Face model ID for Depth Anything V2 (default `depth-anything/Depth-Anything-V2-Small-hf`)
- `MIDAS_ONNX_MODEL_PATH` – defaults to `models/midas_small.onnx`
- `MIDAS_ONNX_PROVIDERS` – comma separated ONNX Runtime providers for depth (falls back to `ONNX_PROVIDERS`)
- `DETECTOR_BACKEND` – `torch` (default) or `onnx`
- `TORCH_DEVICE` – force PyTorch to use `cuda:0`, `cpu`, etc. (defaults to best available)
- `TORCH_HALF_PRECISION` – `auto` (default), `true`, or `false`
- `ONNX_MODEL_PATH` – defaults to `models/yolo11n.onnx`
- `ONNX_OPSET` – opset used during ONNX export (default: 18 via `make export-onnx`)
- `ONNX_SIMPLIFY` – simplify the exported ONNX graph (`true`/`false`, default: true)
- `ONNX_PROVIDERS` – comma separated list such as `CUDAExecutionProvider,CPUExecutionProvider`
- `DETECTOR_IMAGE_SIZE`, `DETECTOR_CONF_THRESHOLD`, `DETECTOR_IOU_THRESHOLD`, `DETECTOR_MAX_DETECTIONS`, `DETECTOR_NUM_CLASSES`
- `MODEL_PATH` (default `models/yolo11n.pt`) – default YOLO model path (used when no CLI flag is provided)
- `VIDEO_FILE_PATH` (default `video.mp4` relative to the `/backend` folder) – default video file path for the file WebRTC service

> Check `src/backend/common/config.py`.


### Calibrate depth and XYZ
- Set camera intrinsics: if you have calibrated values, export them to env vars (pixels): `CAMERA_FX`, `CAMERA_FY`, `CAMERA_CX`, `CAMERA_CY`. If not, set approximate FOVs: `CAMERA_FOV_X_DEG=78 CAMERA_FOV_Y_DEG=65` (defaults). Intrinsics are derived from the first frame size plus these values.
- Calibrate scale for MiDaS: place a target at a known distance `D_true` straight ahead, read the reported distance `D_est`. Update `SCALE_FACTOR` using `SCALE_FACTOR_new = SCALE_FACTOR_old * (D_true / D_est)`, then restart the analyzer. Repeat once or twice until Z is correct; X/Y will align automatically.
- Optional: to log the intrinsics resolved at runtime, set `LOG_INTRINSICS=true` on the analyzer process.


> IMPORTANT: Please read the `CONTRIBUTING.md`.
