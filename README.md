<!--
SPDX-FileCopyrightText: 2025 robot-visual-perception

SPDX-License-Identifier: CC-BY-4.0
-->

# OptiBot

Minimal real-time object and distance detection via YOLO on a WebRTC video stream.

## Quick Start (Windows)

For Windows users, we provide an automated setup script:
```powershell
PowerShell -ExecutionPolicy Bypass -File .\start-optibot.ps1
```

This script will:
- Install all required dependencies (Python 3.11, Node.js, Git, Make, uv)
- Build the project
- Start all services automatically
- Open your browser to http://localhost:3000

**Requirements:** Administrator privileges

For manual setup or other platforms, see the sections below.

## What’s included
- FastAPI + aiortc backend streaming webcam frames
- YOLOv8n inference (Ultralytics) on the server
- Rough monocular distance estimate per detection
- WebRTC DataChannel sending metadata to the client
- React + Vite frontend showing the remote stream and detection stats

## Run with Docker Compose (Linux only)

**Note:** Camera access in Docker only works on Linux.

```bash
make docker-compose-up
```

## Run locally (All platforms)

Prereqs: Python 3.11

1) Install dependencies
```
make dev
```

2) Start the webcam service
```
make run-webcam-local
```
3) Start the analyzer service (separate terminal)
```
make run-analyzer-local
```
The first analyzer start will download `yolov8n.pt` automatically (this will take some time)

### Analyzer Service Options

The analyzer service supports different modes for model management:

**Development mode (default with `make run-analyzer-local`):**
- Automatically downloads and caches models if they don't exist
- Uses `--dev` flag to enable automatic model downloading
- Models are cached for subsequent runs

**Production mode:**
- Uses pre-downloaded models from specified paths
- No network access required at runtime
- Suitable for containerized deployments

Example development usage:
```bash
cd src/backend && uv run python -m analyzer.cli \
  --dev \
  --yolo-model-path ./models/yolov8n.pt \
  --midas-model-path ./models/midas_cache
```

Example production usage:
```bash
cd src/backend && uv run python -m analyzer.cli \
  --yolo-model-path ./models/yolov8n.pt \
  --midas-model-path ./models/midas_cache
```

Available CLI flags:
- `--yolo-model-path`: Path to YOLO model file (e.g., `yolov8n.pt`)
- `--midas-model-path`: Path to MiDaS model cache directory
- `--dev`: Enable development mode (auto-downloads models if not cached)
- `--host`: Host to bind to (default: `0.0.0.0`)
- `--port`: Port to bind to (default: `8001`)
- `--reload`: Enable auto-reload for development

### Environment Variables

Optional environment variables:
- `CAMERA_INDEX` (default 0) – select webcam device
- `REGION_SIZE` (default 5) – size of the central bounding box region where we take the mean of the depth map from (should be odd for symmetry)
- `SCALE_FACTOR` (default 432.0) – scaling of the relative depth map generated by MiDaS (must be determined empirically)
- `MODEL_PATH` (default `models/yolov8n.pt`) – default YOLO model path (used when no CLI flag is provided)

> Check `src/backend/common/config.py`.

## Run frontend
Prereqs: Node 20.

1) Install deps
```
make dev
```
> `make dev` installs both frontend and backend.

2) Start dev server
```
make run-frontend-local
```
Open the shown URL in your console.

## Notes
- The webcam service mirrors and streams raw frames only; the analyzer handles YOLO inference and overlays.
- Analyzer inference is throttled to ~10 Hz to keep latency low.
- Set `TORCH_DEVICE=cuda:0` (or `TORCH_DEVICE=cpu`) before `make run-analyzer-local` to pin PyTorch to a device. When CUDA/ROCm drivers are installed this unlocks GPU acceleration without dropping frames.
- Switch to ONNX Runtime (`DETECTOR_BACKEND=onnx`) for production-style inference. Export a model once via:

  ```bash
  make export-onnx
  ```

  Or manually with uv (use opset 18+ to avoid `Resize` downgrade failures):

  ```bash
  uv run python - <<'PY'
from ultralytics import YOLO
YOLO("models/yolov8n.pt").export(format="onnx", opset=18, imgsz=640, simplify=True)
PY
mv yolov8n.onnx models/yolov8n.onnx
  ```

  Then launch the analyzer with GPU providers, e.g.:

  ```bash
  ONNX_PROVIDERS=CUDAExecutionProvider,CPUExecutionProvider \
  DETECTOR_BACKEND=onnx \
  make run-analyzer-local
  ```

  For ROCm replace the providers with `ROCMExecutionProvider,CPUExecutionProvider` and install the matching `onnxruntime-rocm` wheel. When no provider is specified the service auto-selects the best option reported by ONNX Runtime.

> IMPORTANT: Please read the `CONTRIBUTING.md`.
