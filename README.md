<!--
SPDX-FileCopyrightText: 2025 robot-visual-perception

SPDX-License-Identifier: CC-BY-4.0
-->

# OptiBot

Minimal real-time object and distance detection via YOLO on a WebRTC video stream.

## Quick Start (Windows)

For Windows users, we provide an automated setup script:
```powershell
PowerShell -ExecutionPolicy Bypass -File .\start-optibot.ps1
```

This script will:
- Install all required dependencies (Python 3.11, Node.js, Git, Make, uv)
- Build the project
- Start all services automatically
- Open your browser to http://localhost:3000

**Requirements:** Administrator privileges

For manual setup or other platforms, see the sections below.

## What’s included
- FastAPI + aiortc backend streaming webcam frames
- YOLOv8n inference (Ultralytics) on the server
- Rough monocular distance estimate per detection
- WebRTC DataChannel sending metadata to the client
- React + Vite frontend showing the remote stream and detection stats

## Run with Docker Compose (Linux only)

**Note:** Camera access in Docker only works on Linux.

```bash
make docker-compose-up
```

## Run locally (All platforms)

Prereqs: Python 3.11

1) Install dependencies
```
make dev
```

2) Start the webcam service
```
make run-streamer-webcam
```
Or alternatively, if you provide a .mp4 file in the `/backend` folder per default, you can start the file service instead
```
make run-streamer-file
```
3) Start the analyzer service (separate terminal)
```
make run-analyzer-local
```
The first analyzer start will download `yolo11n.pt` automatically (this will take some time)

### Analyzer Service Options

The analyzer service supports different modes for model management:

**Development mode (default with `make run-analyzer-local`):**
- Automatically downloads and caches models if they don't exist
- Uses `--dev` flag to enable automatic model downloading
- Models are cached for subsequent runs

**Production mode:**
- Uses pre-downloaded models from specified paths
- No network access required at runtime
- Suitable for containerized deployments

Example development usage:
```bash
cd src/backend && uv run python -m analyzer.cli \
  --dev \
  --yolo-model-path ./models/yolo11n.pt \
  --midas-model-path ./models/midas_cache
```

Example production usage:
```bash
cd src/backend && uv run python -m analyzer.cli \
  --yolo-model-path ./models/yolo11n.pt \
  --midas-model-path ./models/midas_cache
```

Available CLI flags:
- `--yolo-model-path`: Path to YOLO model file (e.g., `yolo11n.pt`; `yolov8n.pt` still works)
- `--midas-model-path`: Path to MiDaS model cache directory
- `--dev`: Enable development mode (auto-downloads models if not cached)
- `--host`: Host to bind to (default: `0.0.0.0`)
- `--port`: Port to bind to (default: `8001`)
- `--reload`: Enable auto-reload for development

### Environment Variables

Optional environment variables:
- `CAMERA_INDEX` (default 0) – select webcam device
- `REGION_SIZE` (default 5) – size of the central bounding box region where we take the mean of the depth map from (should be odd for symmetry)
- `SCALE_FACTOR` (default 432.0) – scaling of the relative depth map generated by MiDaS (must be determined empirically)
- `CAMERA_FX/FY/CX/CY` – intrinsic matrix entries in pixels (set these when you have calibrated your camera; overrides FOV-derived values)
- `CAMERA_FOV_X_DEG/CAMERA_FOV_Y_DEG` – fallback field of view (used only when FX/FY are not provided)
- `DEPTH_BACKEND` – `torch` (default) or `onnx`
- `MIDAS_MODEL_TYPE` – MiDaS variant to load (`MiDaS_small`, `DPT_Hybrid`, `DPT_Large`)
- `MIDAS_MODEL_REPO` – torch.hub repo for MiDaS (default `intel-isl/MiDaS`)
- `MIDAS_ONNX_MODEL_PATH` – defaults to `models/midas_small.onnx`
- `MIDAS_ONNX_PROVIDERS` – comma separated ONNX Runtime providers for depth (falls back to `ONNX_PROVIDERS`)
- `DETECTOR_BACKEND` – `torch` (default) or `onnx`
- `TORCH_DEVICE` – force PyTorch to use `cuda:0`, `cpu`, etc. (defaults to best available)
- `TORCH_HALF_PRECISION` – `auto` (default), `true`, or `false`
- `ONNX_MODEL_PATH` – defaults to `models/yolo11n.onnx`
- `ONNX_OPSET` – opset used during ONNX export (default: 18 via `make export-onnx`)
- `ONNX_SIMPLIFY` – simplify the exported ONNX graph (`true`/`false`, default: true)
- `ONNX_PROVIDERS` – comma separated list such as `CUDAExecutionProvider,CPUExecutionProvider`
- `DETECTOR_IMAGE_SIZE`, `DETECTOR_CONF_THRESHOLD`, `DETECTOR_IOU_THRESHOLD`, `DETECTOR_MAX_DETECTIONS`, `DETECTOR_NUM_CLASSES`
- `MODEL_PATH` (default `models/yolo11n.pt`) – default YOLO model path (used when no CLI flag is provided)
- `VIDEO_FILE_PATH` (default `video.mp4` relative to the `/backend` folder) – default video file path for the file WebRTC service

> Check `src/backend/common/config.py`.

## Run frontend
Prereqs: Node 20.

1) Install deps
```
make dev
```
> `make dev` installs both frontend and backend.

2) Start dev server
```
  make run-frontend-local
```
Open the shown URL in your console.

### Calibrate depth and XYZ
- Set camera intrinsics: if you have calibrated values, export them to env vars (pixels): `CAMERA_FX`, `CAMERA_FY`, `CAMERA_CX`, `CAMERA_CY`. If not, set approximate FOVs: `CAMERA_FOV_X_DEG=78 CAMERA_FOV_Y_DEG=65` (defaults). Intrinsics are derived from the first frame size plus these values.
- Calibrate scale for MiDaS: place a target at a known distance `D_true` straight ahead, read the reported distance `D_est`. Update `SCALE_FACTOR` using `SCALE_FACTOR_new = SCALE_FACTOR_old * (D_true / D_est)`, then restart the analyzer. Repeat once or twice until Z is correct; X/Y will align automatically.
- Optional: to log the intrinsics resolved at runtime, set `LOG_INTRINSICS=true` on the analyzer process.

## Notes
- The webcam service mirrors and streams raw frames only; the analyzer handles YOLO inference and overlays.
- Analyzer inference is throttled to ~10 Hz to keep latency low.
- Set `TORCH_DEVICE=cuda:0` (or `TORCH_DEVICE=cpu`) before `make run-analyzer-local` to pin PyTorch to a device. When CUDA/ROCm drivers are installed this unlocks GPU acceleration without dropping frames.
- Switch to ONNX Runtime (`DETECTOR_BACKEND=onnx`) for production-style inference. Export a model once via:

  ```bash
  make export-onnx
  ```

  Or manually with uv (use opset 18+ to avoid `Resize` downgrade failures):

  ```bash
  uv run python - <<'PY'
from ultralytics import YOLO
YOLO("models/yolo11n.pt").export(format="onnx", opset=18, imgsz=640, simplify=True)
PY
mv yolo11n.onnx models/yolo11n.onnx
  ```

  Then launch the analyzer with GPU providers, e.g.:

  ```bash
  ONNX_PROVIDERS=CUDAExecutionProvider,CPUExecutionProvider \
  DETECTOR_BACKEND=onnx \
  make run-analyzer-local
  ```

  For ROCm replace the providers with `ROCMExecutionProvider,CPUExecutionProvider` and install the matching `onnxruntime-rocm` wheel. When no provider is specified the service auto-selects the best option reported by ONNX Runtime.

  MiDaS depth can also run on ONNX Runtime. Export once via:

  ```bash
  make export-midas-onnx
  ```

  Then set `DEPTH_BACKEND=onnx` (optionally `MIDAS_ONNX_PROVIDERS=CUDAExecutionProvider,CPUExecutionProvider`) when starting the analyzer.

> IMPORTANT: Please read the `CONTRIBUTING.md`.
